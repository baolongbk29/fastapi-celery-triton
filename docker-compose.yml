version: "3"

services:
    redis:
        image: redis
        command: redis-server --protected-mode no
        ports:
        - 6379:6379

    worker:
        build: .
        image: triton-benchmark
        command:  celery -A worker.celery worker --loglevel=INFO
        volumes:
            - .:/app
        environment:
            - BROKER_URL=redis://redis:6379/0
            - BACKEND_URL=redis://redis:6379/0
            - TRITON_SERVER_URL=triton-inference-server:8000
        depends_on:
            - redis

    triton_server:
        image: nvcr.io/nvidia/tritonserver:23.06-py3
        container_name: triton-inference-server
        command: tritonserver --model-repository=/models
        volumes:
          - ./server:/models
        ports:
          - 8000:8000
          - 8001:8001
          - 8002:8002
    api:
        build: .
        image: triton-benchmark
        ports:
            - "9000:9000"
        command:  gunicorn -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:9000 --log-level debug app.app:app
        volumes:
        - .:/app
        environment:
        - BROKER_URL=redis://redis:6379/0
        - BACKEND_URL=redis://redis:6379/0
networks:
    default:
        name: mlops-network
